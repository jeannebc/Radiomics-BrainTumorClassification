{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2npMmVzzSM5-"
   },
   "source": [
    "# TP2: MACHINE LEARNING APPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7-DWgNWSM6F"
   },
   "source": [
    "Following the previous notebook, we will go over the fundamentals of machine learning applied to a classification task : binary classification between Low Grade Glioblastoma (LGG) and High Grade Glioblastoma (HGG). Of course, the aim is not to find the best pipeline with the best algorithm for this task but to understand and experiment with the concepts. First, download necessary materials for the practical session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Qc9iPlKTFG1"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jeannebc/Radiomics-BrainTumorClassification.git\n",
    "%cd Radiomics-BrainTumorClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Xr5AtWvSM6L"
   },
   "source": [
    "### Installation of python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDhXnJMmSM6P",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfe1L4HDSM6j"
   },
   "source": [
    "\n",
    "## General Machine Learning Steps\n",
    "\n",
    "Before we start, let's review the basic steps of machine learning:\n",
    "\n",
    "1. Data collection, preprocessing (e.g., integration, cleaning, etc.), and exploration; Splitting a dataset into the **training** and **testing** sets\n",
    "2. Model development:  \n",
    "    A. Let us define a model $\\{f\\}$ as is a collection of candidate functions $f_{w}$. Let's assume that each $f_{w}$ is parametrized by ${w}$  \n",
    "    B. Let us define a **cost function** $C({w})$ which quantifies \"how well a particular $f_{w}$ can explain the training data.\" The lower the cost function the better;  \n",
    "3. **Training:** employ an algorithm that finds the best (or good enough) function $f^{*}$ in the model that minimizes the cost function over the training dataset.\n",
    "4. **Testing**: evaluate the performance of the learned $f^{*}$ using the testing dataset;\n",
    "5. Apply the model in the real world !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxO1e7MiSM6o"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SS9HJy5SM6r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "path_dataset = './data/radiomics_analysis_cleaned.csv'\n",
    "\n",
    "data = pd.read_csv(path_dataset)\n",
    "# we will only work with the full area segmentation with all sequences\n",
    "data = data[data['segmentation']=='full']\n",
    "\n",
    "data = data.pivot_table(index=['patient', 'label'],\n",
    "                                columns=['sequence', 'segmentation'],\n",
    "                                values=data.columns[4:])\n",
    "data.columns = ['_'.join(col).strip() for col in data.columns.values]\n",
    "data.reset_index(level=1, inplace=True)\n",
    "\n",
    "display(data)\n",
    "\n",
    "# Convert LGG into class 0 and HGG into class 1\n",
    "data.loc[data['label'] == 'HGG', 'label'] = 1\n",
    "data.loc[data['label'] == 'LGG', 'label'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfw2r-YMSM64"
   },
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqgZYh-PSM6-"
   },
   "source": [
    "Let’s now use the <code>train_test_split</code> function from scikit-learn to divide feature data (x_data) and target data (y_data, 0 or 1) even further into train and test cohorts. Here we will have 30% of the data for the test set. It is also a good practice to define a random state for reproducible results.\n",
    "\n",
    "Note: the <code>stratify</code> parameter in the function ensures proportions are maintained in the split. For example, if variable y is a binary categorical variable with values 0 and 1, and there are 25% of zeros and 75% of ones, <code>stratify=y_data</code> will make sure that the proportions 75% and 25% are also verified for the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAbEv0V1SM7C"
   },
   "outputs": [],
   "source": [
    "x_data, y_data = data.drop(columns='label'), data['label'].astype(int).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m0rrTwySM7P"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data ,test_size = 0.3, random_state=123, stratify=y_data)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd76_qCqSM7X"
   },
   "source": [
    "We now have our training set to fit and validate our model. The test set is considered like an unseen set and **will be used for performance evaluation only**. Indeed, to be relevant, model evaluation has to be done on data that your model hasn’t seen before.\n",
    "\n",
    "Nevertheless, the strategy to evaluate you model depends on your goal and the chosen approache :\n",
    "\n",
    " - **Scenario 1: Running a simple training**  \n",
    "Split the dataset into separate training and testing sets. Train the model on the former, evaluate the model on the latter. Evaluation is done by a variety of performance metrics such as mean error, precision, recall, ROC auc, etc.\n",
    "\n",
    " - **Scenario 2: Training a model and tuning (optimize) its hyperparameters.**  \n",
    "Split the dataset into separate training and validation sets. Use techniques such as k-fold cross-validation on the training set to find the “optimal” set of hyperparameters for the model. After hyperparameter tuning, use the independent test set to get an unbiased estimate of its performance.\n",
    "\n",
    " - **Scenario 3: Comparing multiple models to identify the best architecture (e.g., SVM vs. logistic regression vs. Random Forests, etc.).**  \n",
    "In this case, we use nested or double cross-validation which leverages an inner and an outer k-fold validation loop. The inner loop chooses the best model and tunes hyperparameters. The outer loop evaluates the resulting choice on unseen folds. Once the optimal model is identified, we evaluate it on the held out test set.\n",
    "\n",
    "![Training options](https://raw.githubusercontent.com/jeannebc/Radiomics-BrainTumorClassification/main/images/evaluate_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDrDriehSM7a"
   },
   "source": [
    "# 1) Introduction to data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWJ3GFuoSM7c"
   },
   "source": [
    "Data pre-processing is an crucial step in machine learning because the quality of the data and it's interpretability directly affects the model's ability to learn useful information.\n",
    "\n",
    "A researcher in AI will spend almost a majority of their time on data cleaning and processing. You will often hear : **Garbage in, garbage out !**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VDfhJcdSM7g"
   },
   "source": [
    "### Handling NULL and NaN (=Not A Number) Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoOngG37SM7l"
   },
   "source": [
    "In any real world dataset there are null values. Tipically, models cannot handle these NULL or NaN values on their own so they need to be edited out of the data. In python a NULL value is represented with NaN, which stands for *Not a Number* (for instance, a division by zero will result in NaN).\n",
    "\n",
    "The first step is to identify potential NULL values in our data using the <code>isnull()</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8ljn4kESM7n"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()\n",
    "# Returns the column names along with the number of NaN values in that particular column (we can specify the axis=1, if we want rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4-SLvDuZNhK"
   },
   "outputs": [],
   "source": [
    "print(data.isnull().sum().any())# add .any() if you want to identify any NaN values in the sum (returns bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3D0LJ6aSM7x"
   },
   "source": [
    "Luckily, there are no NULL values in our dataset. Nevertheless, let us look at some strategies to handle them just in case ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXDrIYAMSM70"
   },
   "source": [
    "#### 1. Quick and easy : dropping (i.e. removing) the rows or columns that contain NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRlSJpc5SM74"
   },
   "outputs": [],
   "source": [
    "data.dropna(); # (axis=0 for columns or 1 for rows), you have various parameters for dropna(), like 'how', 'tresh',\n",
    "# take a look at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGXuiAzgSM8F"
   },
   "source": [
    "#### 2. More evolved : imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQXghf68SM8K"
   },
   "source": [
    "Imputation is defined by the substitution of the missing values of our dataset. The NULL values can be replaced by mean, max, 0, a custom function ... We can even train another algorithm to predict the values of the missing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIOQk6_6SM8N"
   },
   "outputs": [],
   "source": [
    "data.fillna(0); # will replace NaN values by a 0, take a look at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFsJ1qsaSM8W"
   },
   "source": [
    "### Standardization/Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzvchF4BSM8Y"
   },
   "source": [
    "Normalization rescales the data to ensure the values of different features fall in the specified range, e.g. [0,1].\n",
    "\n",
    "Indeedn different radiomics features have different units and range. Some features are designed to be in range [0,1], while others can have a very large range. In most machine learning algorithms, the objective function will not give relevant results without normalization. For example, many classifiers calculate the distance between two points using the Euclidean distance. If one of the features has a wide range of values, the distance will be governed by that particular characteristic as all other distances will become negligible. Normalization ensures each feature contributes equally to the final distance. Let us introduce the most common approaches to normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv0_apFPSM9H"
   },
   "source": [
    "#### 1. Min-Max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adVmLPHySM9K"
   },
   "source": [
    "This estimator scales and translates each feature individually such that it is in the desired range, e.g. between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR2BWfuOSM9M"
   },
   "source": [
    "$$ X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDPgsDapSM89"
   },
   "source": [
    "Scikit-learn directly implements this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV5-z_ZcSM9P"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_standardize = scaler.transform(x_train)\n",
    "# We apply the same transform on the test\n",
    "x_test_standardize = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRHtAYJ_SM8a"
   },
   "source": [
    "#### 2. Z-Score normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owgtxxi3SM8d"
   },
   "source": [
    "It standardize features by removing the mean and scaling to unit variance. In other words, we transform our values such that the mean of the values is 0 and the standard deviation is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylhGX9VhSM8i"
   },
   "source": [
    "$$\n",
    "Z = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$  \n",
    "with mean: $$\\mu = \\frac{1}{N} \\sum_{i=1}^N (x_i)$$\n",
    "and standard deviation: $$\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CMVvdasSM8l"
   },
   "source": [
    "**Exercice:**\n",
    "Complete the following scripts to:\n",
    "\n",
    "- do the Z-Score normalization on the <code>x_train</code> and apply it on the  <code>x_test</code> without using the scikit-learn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETwe0kjiSM8n"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 1\n",
    "\n",
    "# How z-score normalization is implemented?\n",
    "\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def z_score(X):\n",
    "    # zero mean and unit variance\n",
    "    mean =  '''CompleteHere'''\n",
    "    std_dev =  '''CompleteHere'''\n",
    "    z =  '''CompleteHere'''\n",
    "    return z, mean, std_dev\n",
    "\n",
    "\n",
    "x_train_standardize, mean, std_dev = '''CompleteHere'''(x_train)\n",
    "x_test_standardize = (('''CompleteHere''' - mean) / std_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXqwOyl4SM9A"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_standardize = scaler.transform(x_train)\n",
    "# We apply the same transform on the test set\n",
    "x_test_standardize = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUVFMjKiSM9a"
   },
   "source": [
    "There are many other techniques, you can consult this link which\n",
    " <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\">compares the effect of different scalings on data with outliers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jWYanDHSM9d"
   },
   "source": [
    "# 2) Building models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poBXP9oHSM9q"
   },
   "source": [
    "#### Standard validation (Hold-Out method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY-YNBRnSM9h"
   },
   "source": [
    "Previously, we have separated our data into train and test sets. We used the complete training set to derive estimates of means and variances for each feature in order to perform Z-score normalization.\n",
    "\n",
    "We now further split the training set into a smaller training set and a validation set. The training set samples will all be used to optimize the parameters of a model (i.e. iteratively converge to a satisfying model from the input family of functions), and the validation set will be used to select a good set of hyper-parameters such as the model type (here, logestic regression, decision trees, support vector machines).\n",
    "\n",
    "The function `train_test_split()` from scikit-learn is also used on the pair (features, labels) of the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqwJKGFKSM9r"
   },
   "source": [
    "![Training options](https://raw.githubusercontent.com/jeannebc/Radiomics-BrainTumorClassification/main/images/holdout_method.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHDE-6jySM9j"
   },
   "outputs": [],
   "source": [
    "train_features, validation_features, train_labels, validation_labels = \\\n",
    "  train_test_split(x_train, y_train ,test_size = 0.3, random_state=123, stratify=y_train)\n",
    "\n",
    "print('train_features shape: ', train_features.shape)\n",
    "print('validation_features shape: ', validation_features.shape)\n",
    "print('train_labels shape: ', train_labels.shape)\n",
    "print('validation_labels shape: ', validation_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-U0d1dX_X8lg"
   },
   "source": [
    "#### Example : building a model with `scikit-learn`\n",
    "\n",
    "Because od its simplicity of usage and consistency, `scikit-learn` (or `sklearn`) has become the leading data science library. It implements a wide range of ML and data processing algorithms. Let us look at some examples of models and training pipelines from `sklearn`.\n",
    "\n",
    "Logistic regression is a very popular classifier for applications to the medical field. It models the relationship between a categorical response variable $y$ and a set of $x \\in R^k$ of $k$ features per input by fitting a linear equation. The goal of training is then to find the weights or coefficients in the linear function such that the output is closest to the real label.\n",
    "\n",
    "While all of the training pipeline of logistic regression can be hand-coded, its implementation in `sklearn` fits in two lines only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX-OZ4MdYuV4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression_model = LogisticRegression()  # instantiate a logistic regression model with default parameters\n",
    "print(logistic_regression_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjehFO2EaVOb"
   },
   "source": [
    "Training is then performed using the `fit` function, which applies gradient descent with input hyper-parameters, and usually stops once a training hyper-parameter is satisfied, such as minimal training error tolerance or number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKJFqk-Eahp6"
   },
   "outputs": [],
   "source": [
    "clf = logistic_regression_model.fit(X=train_features, y=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY9TZ2XXtOs3"
   },
   "source": [
    "##### Performance assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0HYYrU0aoFD"
   },
   "source": [
    "Once the model is trained, i.e. `fit` function is done running, it can be used to classify any input sample with the correct shape, or vectors of 140 features in this case. Notably, training and validation accuracies can be obtained with other performance indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh8ki8RKbBqT"
   },
   "outputs": [],
   "source": [
    "accuracy_train = clf.score(X=train_features, y=train_labels)\n",
    "probs = clf.predict_proba(train_features)\n",
    "accuracy_validation = clf.score(X=validation_features, y=validation_labels)\n",
    "print('Training accuracy', accuracy_train, '; Validation accuracy', accuracy_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGQ4T0VRbbSl"
   },
   "source": [
    "In our problem setting, the dataset contains more positive samples than negative samples. This implies that a model outputting only the positive class would result in an accuracy above 50%. In other words, the accuracy may not always be suited to the needs of the task in hand. Other performance metrics, such as balanced accuracy (https://en.wikipedia.org/wiki/Precision_and_recall) are implemented in scikit-learn (https://scikit-learn.org/stable/modules/model_evaluation.html). All implemented performance metrics take two vectors as their inputs, one being the model predictions and the other ground-truths. We first need to explicitely compute the predictions of the trained model and then run sklearn metrics functions:\n",
    "\n",
    "*Note : here, model predictions are represented by a vector of size 2. The first number represents the probability of the patients being in category 1, and the second one the probability of the patient being in category 2. These two probabilities always sum up to 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-H1lcRrcOwS"
   },
   "outputs": [],
   "source": [
    "# Use the trained model (clf) to explicitely compute probabilities of all training and validation samples\n",
    "predicted_training_probabilities = clf.predict_proba(train_features)[:, 1]  # for each sample, two outputs probas summing to 1: one for class 0 (LGG), the other for class 1 (HGG)\n",
    "predicted_validation_probabilities = clf.predict_proba(validation_features)[:, 1]  # for each sample, two outputs probas summing to 1: one for class 0 (LGG), the other for class 1 (HGG)\n",
    "\n",
    "# Compute predicted classes from predicte?d probabilities by thresholded probabilities with 0.5: a probability higher than 0.5 would yield HGG prediction, otherwise LGG prediction\n",
    "training_predicted_classes = list(map(lambda proba: int(proba > .5), predicted_training_probabilities))\n",
    "validation_predicted_classes = list(map(lambda proba: int(proba > .5), predicted_validation_probabilities))\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "# Metrics function expect first the ground-truth vector, then the predicted probabilities/classes one\n",
    "training_balanced_accuracy = balanced_accuracy_score(train_labels, training_predicted_classes)\n",
    "validation_balanced_accuracy = balanced_accuracy_score(validation_labels, validation_predicted_classes)\n",
    "print('Training balanced accuracy', training_balanced_accuracy, '; Validation balanced accuracy', validation_balanced_accuracy)\n",
    "\n",
    "training_auc = roc_auc_score(train_labels, predicted_training_probabilities)\n",
    "validation_auc = roc_auc_score(validation_labels, predicted_validation_probabilities)\n",
    "print('Training AUC', training_balanced_accuracy, '; Validation AUC', validation_balanced_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2rJh1Zedijk"
   },
   "source": [
    "Although the balanced accuracy alleviates the issue of class imbalance, it is not a means of assessing the performance of a decision system. Indeed, performance should be assessed using two measures, such as precision and recall. While giant technology companies can make mistakes in online tools such as Facebook when suggesting friends to tag on newly uploaded pictures, in medical routine tasks, errors can have a significant impact on patient care or material maintenance. For instance, false positives (a test finds that a patient is sick with cancer when in reality the patient is healthy) have much less impact than false negatives (a patient's cancer goes undetected). The community would expect guarantees that the level of false negative is close to none for diagnostic tasks, even if this implies a significant amount of false positive.\n",
    "\n",
    "**Exercice:**\n",
    "Complete the following script to:\n",
    "\n",
    "- evaluate the logistic regression model trained above, using sklearn documentation (https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- compute the precision (also called positive predictive value) and recall (also called true positive rate or sensitivity) called  of the trained classifier on both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0RlCyKIfIAe"
   },
   "outputs": [],
   "source": [
    "#@title Exercise 2\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "training_precision = '''CompleteHere'''(train_labels, training_predicted_classes)\n",
    "training_recall = '''CompleteHere'''('''CompleteHere''', '''CompleteHere''')\n",
    "\n",
    "validation_precision = precision_score(validation_labels, validation_predicted_classes)\n",
    "validation_recall = recall_score(validation_labels, validation_predicted_classes)\n",
    "\n",
    "print('Training precision', '''CompleteHere''', '; training recall', '''CompleteHere''')\n",
    "print('Validation precision', '''CompleteHere''', '; Validation recall', '''CompleteHere''')\n",
    "\n",
    "#Note: pays attention that the two metrics functions take predicted classes as input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVYrzrz2tuIm"
   },
   "source": [
    "##### Analysing the trained model\n",
    "\n",
    "Fundamentally, logistic regression is parametrized by two parameters per input feature : the weight (or coefficient) and the bias (or intercept). Intuitively, features corresponding to weights with high weights will have more influence on the output that those with weight close to 0.\n",
    "\n",
    "Generally, the parameters of any classifier in scikit-learn can be extracted using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ5H9dzluyXZ"
   },
   "outputs": [],
   "source": [
    "# one parameter per input feature + one for intercept with 0\n",
    "logistic_regression_parameters = clf.coef_\n",
    "logistic_regression_intercept = clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeiO7-RYvQmt"
   },
   "source": [
    "Let us pair each parameter with its associated feature name and sort the parameters by magnitude. Then we retrieve the top 10 with most important magnitude i.e. **the 10 most relevant features for our classification task** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le9N0uoUvXZM"
   },
   "outputs": [],
   "source": [
    "# Pair each parameter with its associated feature name\n",
    "logistic_regression_parameters_with_names = list(zip(logistic_regression_parameters[0], x_data.columns.values))\n",
    "print('Example of paires param/feature name', logistic_regression_parameters_with_names[:3])\n",
    "print('Intercept value', logistic_regression_intercept)\n",
    "\n",
    "# Sort paired data with respect to absolute value of parameters\n",
    "logistic_regression_parameters_with_names = sorted(logistic_regression_parameters_with_names, key=lambda pair: abs(pair[0]))\n",
    "\n",
    "print('\\nMost important features:')\n",
    "# Select top 10 max magnitude parameters and print associated feature name\n",
    "for parameter_value, feature_name in logistic_regression_parameters_with_names[:-10:-1]:\n",
    "  print('\\t\\t', feature_name, 'with value:', str(parameter_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6z39Ii5dyHrH"
   },
   "source": [
    "So far, we trained a logistic regression model, looked at some metrics to assess its performance, and looked into the trained parameters to infer the most important features found by the model.\n",
    "\n",
    "In a typical ML setting, we would then try to further boost the performance of our decision system by trying different parameters or by using another family of functions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TwavqnXLgq5"
   },
   "source": [
    "### Improving performance with hyper-parameter optimization\n",
    "\n",
    "In the previous section, we implemented a logistic regression model using scikit-learn for the classification of MRI images into LGG or HGG. However, there is no guarantee that logistic regression is the best suited family of functions for this task, with this type of data. There are many families of machine learning decision systems, the behavior and performance of which depend on the experiment context. Let's take a look at other options !\n",
    "\n",
    "First, we will train 6 algorithms and use the reported generalization performance on the validation set to extract the best performing family of models. We will also look into another hyper-parameter which is the standardization of the data, and compare the validation generalization performance of models trained with standardization with respect to those trained without to determine whether applying standardization is beneficial to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hjg6JXr70Xg"
   },
   "source": [
    "#### Standard validation (Hold-Out method : we split the training set into a train and a validation set)\n",
    "\n",
    "We observed that how it worked above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw1o-RgfSM98"
   },
   "source": [
    "##### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCtzGVXNSM9-"
   },
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def standardize(x_train, x_test, type='zscore'):\n",
    "    if type == 'zscore':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train_standardize = scaler.transform(x_train)\n",
    "    # We apply the same transform on the test\n",
    "    x_test_standardize = scaler.transform(x_test)\n",
    "    return x_train_standardize, x_test_standardize\n",
    "\n",
    "\n",
    "random_state = 1234\n",
    "\n",
    "# We load 6 different algorithms with associated algorithm name\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(random_state=random_state)))\n",
    "models.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n",
    "models.append(('K-nearest neighbours', KNeighborsClassifier()))\n",
    "models.append(('Decision Tree Classifier', DecisionTreeClassifier(random_state=random_state)))\n",
    "models.append(('Gaussian Naive Bayes', GaussianNB()))\n",
    "models.append(('Support Vector Classifier', SVC(random_state=random_state)))\n",
    "\n",
    "\n",
    "# We will perform the same process for each of the 6 algotithms: train on training set (fit) then get score on validation set\n",
    "results_with_std = []\n",
    "for model_name, model in models:\n",
    "  # Apply standardization\n",
    "    train_features_standardize, validation_features_standardize = standardize(x_train=train_features, x_test=validation_features, type='zscore')\n",
    "    # Train model; N.B.: training features not standardized were train_features\n",
    "    clf = model.fit(X=train_features_standardize, y=train_labels)\n",
    "    accuracy_train = clf.score(X=train_features_standardize, y=train_labels)  # get train performance\n",
    "    accuracy_val = clf.score(X=validation_features_standardize, y=validation_labels) # get validation performance\n",
    "    results_with_std.append(accuracy_val)\n",
    "    print(\"%s: train = %.3f, validation = %.3f\" % (model_name, accuracy_train, accuracy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YJ-dF80R8Ga"
   },
   "source": [
    "Generally, in machine learning, we cannot identify the best model until training is over.\n",
    "\n",
    "Here, the models have been tested only once on the validation set. To make our pipeline more robust, let us run cross validation instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSPs6cu5riNl"
   },
   "source": [
    "#### K-fold cross validation\n",
    "\n",
    "So far, we have :\n",
    "- split our entire dataset into a \"global machine learning optimization\" set and a testing set.\n",
    "- split the \"global machine learning optimization\" set into a training set, used to optimize the parameters of decision systems, and a validation set for hyper-parameters tuning.\n",
    "\n",
    "This second split is random, as there is no reason to privilege some data samples over others. Thus, we could take the \"global machine learning optimization\" set and partition it into a new training set and new associated validation set.\n",
    "\n",
    "K-fold cross-validation is the process of splitting a set into two sets k times, where for each split one set is used to train the model and the second to assess its performance. This yields more robust estimates of model performance without requiring more data.\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "**Exercice:**\n",
    "\n",
    "Complete the script below to implement the K-fold cross validation based on https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "K-fold cross validation is performed as follows:\n",
    "  - Randomly split your entire dataset into \"k-folds\"\n",
    "  - For each fold in your dataset, train your model on the remaining k – 1 folds. Then, assess model performance on the hold-out fold.\n",
    "  - Save the error you see on each of the predictions.\n",
    "  - Repeat this until each of the k-folds has served as the test set.\n",
    "  - The average of your k recorded errors is called the cross-validation error. It is used as your performance metric for the model.\n",
    "\n",
    "\n",
    "  Now, one of the most commonly asked questions is, *“How do I choose the right value of k?”*.\n",
    "\n",
    "A lower value of k is more biased, and hence undesirable. On the other hand, a higher value of k is less biased, but at the cost of an increase in variability.\n",
    "\n",
    "A smaller value of k brings us closer towards the single validation set approach, whereas a higher value of k leads to Leave-One-Out Cross Validation (LOOCV) approach. Actually, LOOCV is equivalent to n-fold cross validation where n is the number of training examples.\n",
    "\n",
    "\n",
    "![CV](https://raw.githubusercontent.com/jeannebc/Radiomics-BrainTumorClassification/main/images/cross_validation_method.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqyQPcQgSM-a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def cross_validation(X, y, model, num_folds=5):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.to_numpy()\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.to_numpy()\n",
    "\n",
    "    cv = KFold(n_splits=num_folds, random_state=123, shuffle=True)\n",
    "    results_train, results_test = [], []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "        clf = model.fit('''CompleteHere''', '''CompleteHere''')\n",
    "        accuracy_train = clf.score(X='''CompleteHere''', y='''CompleteHere''')\n",
    "        accuracy_test = clf.score(X='''CompleteHere''', y='''CompleteHere''')  # Return the mean accuracy\n",
    "        results_train.append(accuracy_train)\n",
    "        results_test.append(accuracy_test)\n",
    "    return '''CompleteHere''', '''CompleteHere'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhKThTz3SM-n"
   },
   "source": [
    "We implement cross validation on the <code>x_train</code> data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Srv4tmGhSM_A"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def standardize(x_train, x_test, type='zscore'):\n",
    "    if type == 'zscore':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train_standardize = scaler.transform(x_train)\n",
    "    # We apply the same transform on the test\n",
    "    x_test_standardize = scaler.transform(x_test)\n",
    "    return x_train_standardize, x_test_standardize\n",
    "\n",
    "\n",
    "def cross_validation_with_standardization(X, y, model, num_folds=5):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.to_numpy()\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        y = y.to_numpy()\n",
    "\n",
    "    cv = KFold(n_splits=num_folds, random_state=123, shuffle=True)\n",
    "    results_train, results_test = [], []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "\n",
    "         # standardize with z-score\n",
    "        X_train, X_test = standardize(x_train=X_train, x_test=X_test, type='zscore')\n",
    "\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        accuracy_train = clf.score(X=X_train, y=y_train)\n",
    "        accuracy_test = clf.score(X=X_test, y=y_test)  # Return the mean accuracy\n",
    "        results_train.append(accuracy_train)\n",
    "        results_test.append(accuracy_test)\n",
    "    return results_train, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIqGmWecSM_H"
   },
   "outputs": [],
   "source": [
    "all_results_cv_train_w_std, all_results_cv_val_w_std, names = [], [], []\n",
    "for name, model in models:\n",
    "    names.append(name)\n",
    "    results_cv_train_w_std, results_cv_val_w_std = cross_validation_with_standardization(X=x_train, y=y_train, model=model, num_folds=5)\n",
    "    all_results_cv_train_w_std.append(results_cv_train_w_std)\n",
    "    all_results_cv_val_w_std.append(results_cv_val_w_std)\n",
    "    means_train, stds_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n",
    "    means_val, stds_val = np.mean(results_cv_val_w_std), np.std(results_cv_val_w_std)\n",
    "    msg = \"%s: train = %.3f (+/- %.3f), test = %.3f (+/- %.3f)\" % (name, means_train, stds_train, means_val, stds_val)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtYq4GsGtLnr"
   },
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.title('Algorithm Comparison using cross validation with standardization')\n",
    "plt.boxplot(all_results_cv_val_w_std)\n",
    "ax.set_ylim([0.60, 0.95])\n",
    "ax.set_xticklabels(names, rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVDwmEvxSM_O"
   },
   "source": [
    "Compare the results of cross validation without standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xIKmvoi-kiJ"
   },
   "outputs": [],
   "source": [
    "def cross_validation_without_standardization():\n",
    "  \"\"\"TODO\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG8--v_2SM_S"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# boxplot algorithm comparison\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "ax = plt.subplot(122)\n",
    "plt.title('Algorithm Comparison using cross validation without standardization')\n",
    "plt.boxplot(\"\"\"COMPLETE HERE\"\"\")\n",
    "ax.set_ylim([0.60, 0.95])\n",
    "ax.set_xticklabels(names, rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HArcwqu-tmR-"
   },
   "source": [
    "### Which model would you choose ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDeFa8mQSM_X"
   },
   "source": [
    "# 3) Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdQxGDfuSM_a"
   },
   "source": [
    "When training a model, it is crucial to avoid two common pitfalls : overfitting or on the contrary, underfitting.\n",
    "\n",
    "- **Overfitting**:  \n",
    "A model suffers from overfitting when it becomes specific to the data it is trained on, and is no longer able to generalize accurately to unseen data. This is usually caused by overtraining the model on the training set. **The bigger the model (i.e. the more parameters), the higher the chances of overfitting**.\n",
    "\n",
    "- **Underfitting**:  \n",
    "Intuitively, a model suffers from underfitting when it has not been trained enough and behaves almost like a randomly initialized model. It is not speciff enough to the data and the task.\n",
    "\n",
    "![over-underfitting](https://raw.githubusercontent.com/jeannebc/Radiomics-BrainTumorClassification/main/images/over_and_under_fitting.png)\n",
    "\n",
    "*How do we detect overfitting?*  \n",
    "A key challenge with overfitting, and with machine learning in general, is that we **do not know how well our model will perform on new data until we actually test it.**\n",
    "\n",
    "To make sure the model overfit, we compare model performance on seen data (training set) versus unseen data (testing set). If our model does much better on the training set than on the testing set, we are likely overfitting. For instance, seeing a 99% training accuracy but a 55% testing accuracy is a clear sign of overfitting. The bigger the gap in performance, the stronger the overfit.\n",
    "\n",
    "*How do we prevent overfitting?*  \n",
    "\n",
    "- **Cross-validation**  \n",
    "As seen previously, it implies using the initial training data to generate multiple mini train-test splits to tune your model.\n",
    "In standard k-fold cross-validation, the data is partitioned into k subsets, called folds. The model is iteratively trained on the k-1 folds, and tested on the remaining one (the holdout fold). Cross-validation allows for hyperparameter tuning without integrating additional data, keeping the test set apart for final model evaluation.\n",
    "\n",
    "- **Remove features**  \n",
    "This is a means of reducing the number of parameters and making the model simpler. Some algorithms have built-in feature selection. For those that do not, you can manually improve their generalizability by removing irrelevant input features.\n",
    "\n",
    "- **Regularization**  \n",
    "Regularization refers to a broad range of techniques for artificially forcing your model to be simpler. The method will depend on the type of learner you are using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "- **Also : additional data, ensembling, early stopping ...**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9falgMUSM_b"
   },
   "source": [
    "# 4) Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zRJ5ypiSM_d"
   },
   "source": [
    "Wikipedia states that \"*hyperparameter tuning/optimizization is choosing a set of optimal hyperparameters for a learning algorithm*\". What exactly is a hyperparameter ?\n",
    "\n",
    "<div align=\"center\">\n",
    "    <i> a hyperparameter is a parameter whose value is set before the learning process begins </i>\n",
    "</div>\n",
    "\n",
    "An example of hyperparameter is include penalty in logistic regression\n",
    "In sklearn, hyperparameters are passed as arguments of the constructor of the models classes.\n",
    "   \n",
    "**Tuning Strategies:**\n",
    " - Grid Search  \n",
    " Also known as an exhaustive search, Grid search looks through each combination of hyperparameters. This means that every combination of specified hyperparameter values will be tried.\n",
    " - Random Search  \n",
    " As its names suggests, Random Search uses random combinations of hyperparameters. This means that not all of the parameter values are tried, and instead, parameters will be sampled with fixed numbers of iterations.\n",
    "\n",
    " ![grid_search](https://raw.githubusercontent.com/jeannebc/Radiomics-BrainTumorClassification/main/images/random_grid_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLSSI_hFSM_f"
   },
   "source": [
    "### Implementing Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8BEr4BvSM_h"
   },
   "source": [
    "Let us define the grid search space. The pipeline implementing z-score normalisation will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yL7AzvrTSM_j"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a dictionary with classifier name as a key and it's hyper parameters options as a value for grid search\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Logistic Regression Params\n",
    "C = [x for x in np.arange(0.1, 3, 0.2)]\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "fit_intercept = [True, False]\n",
    "solver = [\"saga\"]\n",
    "lr_params = {'C': C,\n",
    "             'penalty': penalty,\n",
    "             'fit_intercept': fit_intercept,\n",
    "             'solver': solver\n",
    "             }\n",
    "\n",
    "# DecisionTreeClassifier PARAMS\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "class_weight = [None, \"balanced\"]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "max_features = [None, \"sqrt\", \"log2\"]\n",
    "dtc_params = {'criterion': criterion,\n",
    "              'splitter': splitter,\n",
    "              'class_weight': class_weight,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'max_features': max_features\n",
    "              }\n",
    "\n",
    "# KNN PARAMS\n",
    "n_neighbors = [int(x) for x in np.linspace(start=1, stop=20, num=2)]\n",
    "weights = [\"uniform\", \"distance\"]\n",
    "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "leaf_size = [int(x) for x in np.linspace(start=5, stop=50, num=2)]\n",
    "p = [int(x) for x in np.linspace(start=1, stop=4, num=1)]\n",
    "knn_params = {'n_neighbors': n_neighbors,\n",
    "              'weights': weights,\n",
    "              'algorithm': algorithm,\n",
    "              'leaf_size': leaf_size,\n",
    "              'p': p,\n",
    "              }\n",
    "\n",
    "# LDA PARAMS\n",
    "solver = [\"lsqr\"]\n",
    "shrinkage = [\"auto\", None, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "lda_params = {'solver': solver,\n",
    "              'shrinkage': shrinkage\n",
    "              }\n",
    "\n",
    "# GaussianNB PARAMS\n",
    "var_smoothing = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5] #Portion of the largest variance of all features that is added to variances for calculation stability\n",
    "gnb_params = {'var_smoothing': var_smoothing,\n",
    "              }\n",
    "\n",
    "# SVC PARAMS\n",
    "C = [x for x in np.arange(0.1, 2, 0.2)]\n",
    "gamma = [\"auto\"]\n",
    "kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "degree = [1, 2, 3, 4, 5, 6]\n",
    "svc_params = {'C': C,\n",
    "              'gamma': gamma,\n",
    "              'kernel': kernel,\n",
    "              'degree': degree,\n",
    "              }\n",
    "\n",
    "hypertuned_params_gs = {\"Logistic Regression\": lr_params,\n",
    "                     \"Decision Tree Classifier\": dtc_params,\n",
    "                     \"K-nearest neighbours\": knn_params,\n",
    "                     \"Linear Discriminant Analysis\": lda_params,\n",
    "                     \"Gaussian Naive Bayes\": gnb_params,\n",
    "                     \"Support Vector Classifier\": svc_params,\n",
    "                     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNDsw331SNAD"
   },
   "outputs": [],
   "source": [
    "# grid search function\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def grid_search(X, y, name, model, param_grid, verbose=False):\n",
    "    names = []\n",
    "    names.append(name)\n",
    "    result_gs, max_val_mean_val = [], 0\n",
    "    for i, params in enumerate(param_grid): #enumerate: adds a counter to an iterable object\n",
    "        model = model.set_params(**params)\n",
    "        results_cv_train_w_std, results_cv_val_w_std = cross_validation_with_standadization(X=X, y=y, model=model, num_folds=5)\n",
    "        mean_train, std_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n",
    "        mean_val, std_val = np.mean(results_cv_val_w_std), np.std(results_cv_val_w_std)\n",
    "        if verbose:\n",
    "            print(\"%s - iteration %i: %f (%f)\" % (name, i, mean_val, std_val))\n",
    "        if mean_val > max_val_mean_val:\n",
    "            max_val_mean_test = mean_val\n",
    "            max_val_std_val = std_val\n",
    "            max_val_mean_train = mean_train\n",
    "            max_val_std_train = std_train\n",
    "            max_i = i\n",
    "            best_params = param_grid[i]\n",
    "    msg = \"%s: Maximum value on validation = %.3f (+/- %.3f) with train = %.3f (+/- %.3f) for iteration %i with params: %s\" % (name, max_val_mean_test, max_val_std_val, max_val_mean_train, max_val_std_train, max_i, best_params)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZ0IZh9OEr3Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "for name, model in models:\n",
    "    param_grid = list(ParameterGrid(hypertuned_params_gs[name]))\n",
    "    grid_search(X=x_train, y=y_train, name=name, model=model, param_grid=param_grid, verbose=False)  # you can set verbose to True to see each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DibIT5sO_i1Y"
   },
   "source": [
    "### Exercice 4\n",
    "Compare with previous results with the grid search and without any hyperparameters optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyaTmT2aSNAT"
   },
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CNO78j2SNAX"
   },
   "source": [
    "Let us define the random search space. The pipeline implementing z-score normalisation will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew_T4PGgSNAZ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a dictionary with classifier name as a key and it's hyper parameters options as a value for Random search\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Logistic Regression Params\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "C = uniform(loc=0, scale=4)\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "fit_intercept = [True, False]\n",
    "solver = [\"saga\"]\n",
    "lr_params = {'C': C,\n",
    "             'penalty': penalty,\n",
    "             'fit_intercept': fit_intercept,\n",
    "             'solver': solver\n",
    "             }\n",
    "\n",
    "# DecisionTreeClassifier PARAMS\n",
    "criterion = ['gini', 'entropy']\n",
    "splitter = ['best', 'random']\n",
    "class_weight = [None, \"balanced\"]\n",
    "max_depth = list(range(10, 501))\n",
    "max_depth.append(None)\n",
    "min_samples_split = list(range(2, 101))\n",
    "min_samples_leaf = list(range(1, 50))\n",
    "max_features = [None, \"sqrt\", \"log2\"]\n",
    "dtc_params = {'criterion': criterion,\n",
    "              'splitter': splitter,\n",
    "              'class_weight': class_weight,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'max_features': max_features\n",
    "              }\n",
    "\n",
    "# KNN PARAMS\n",
    "n_neighbors = list(range(1, 101))\n",
    "weights = [\"uniform\", \"distance\"]\n",
    "algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n",
    "leaf_size = list(range(2, 101))\n",
    "p = list(range(1, 11))\n",
    "knn_params = {'n_neighbors': n_neighbors,\n",
    "              'weights': weights,\n",
    "              'algorithm': algorithm,\n",
    "              'leaf_size': leaf_size,\n",
    "              'p': p,\n",
    "              }\n",
    "\n",
    "# LDA PARAMS\n",
    "solver = [\"lsqr\"]\n",
    "shrinkage = [\"auto\", None, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "lda_params = {'solver': solver,\n",
    "              'shrinkage': shrinkage\n",
    "              }\n",
    "\n",
    "# GaussianNB PARAMS\n",
    "var_smoothing = uniform(loc=0, scale=0.1)\n",
    "gnb_params = {'var_smoothing': var_smoothing,\n",
    "              }\n",
    "\n",
    "# SVC PARAMS\n",
    "C =  uniform(loc=0, scale=2)\n",
    "gamma = [\"auto\"]\n",
    "kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "degree = list(range(1,11))\n",
    "svc_params = {'C': C,\n",
    "              'gamma': gamma,\n",
    "              'kernel': kernel,\n",
    "              'degree': degree,\n",
    "              }\n",
    "\n",
    "hypertuned_params_rs = {\"Logistic Regression\": lr_params,\n",
    "                     \"Decision Tree Classifier\": dtc_params,\n",
    "                     \"K-nearest neighbours\": knn_params,\n",
    "                     \"Linear Discriminant Analysis\": lda_params,\n",
    "                     \"Gaussian Naive Bayes\": gnb_params,\n",
    "                     \"Support Vector Classifier\": svc_params,\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11OdRu3WSNAg"
   },
   "outputs": [],
   "source": [
    "# random search function\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def random_search(X, y, name, model, param_grid, nb_iterations, verbose=False):\n",
    "    best_params = []\n",
    "    names = []\n",
    "    names.append(name)\n",
    "    result_rs, max_val_mean_val = [], 0\n",
    "    for i in range(nb_iterations):\n",
    "        # create random param from the grid dict\n",
    "        params = {key: value.rvs() if isinstance(value, type(uniform())) else random.choice(value) for key, value in param_grid.items()}\n",
    "        model = model.set_params(**params)\n",
    "        results_cv_train_w_std, results_cv_val_w_std = cross_validation_with_standadization(X=X, y=y, model=model, num_folds=5)\n",
    "        mean_train, std_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n",
    "        mean_test, std_val = np.mean(results_cv_val_w_std), np.std(results_cv_val_w_std)\n",
    "        if verbose:\n",
    "            print(\"%s - iteration %i: %f (%f)\" % (name, i, mean_test, std_val))\n",
    "        if mean_test > max_val_mean_val:\n",
    "            max_val_mean_test = mean_test\n",
    "            max_val_std_test = std_val\n",
    "            max_val_mean_train = mean_train\n",
    "            max_val_std_train = std_train\n",
    "            max_i = i\n",
    "            best_params = params\n",
    "    msg = \"%s: Maximum value on validation = %.3f (+/- %.3f) with train = %.3f (+/- %.3f) for iteration %i with params: %s\" % (name, max_val_mean_test, max_val_std_test, max_val_mean_train, max_val_std_train, max_i, best_params)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_eGlkYCSNAm"
   },
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    dic_grid = hypertuned_params_rs[name]\n",
    "    random_search(X=x_train, y=y_train, name=name, model=model, param_grid=dic_grid, nb_iterations=100, verbose=False)  # you can set verbose to True to see each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOuy4k5wSNA1"
   },
   "source": [
    "Here, we implemented our own Grid Search and Random Search function to understand the mechanisms at play. In practice, sklearn directly implements the functions, [Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFy8XnzzSNBP"
   },
   "source": [
    "# 5) Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukGBPl70SNBR"
   },
   "source": [
    "The ''No Free Lunch'' theorem states that no machine learning algorithm is universally better than the others in all domains. The goal of ensembling is to combine multiple learners to improve the applicability and achieve better performance. Importantly, if two models have comparable performance, the simplest architecture should be priviledged (see [Occam's razor](https://simple.wikipedia.org/wiki/Occam%27s_razor))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG8LeKCESNBU"
   },
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpuYrDetSNBX"
   },
   "source": [
    "Voting is arguably the most straightforward way to combine multiple learners $d^{(j)}(\\cdot)$. The idea is to take a linear combination of the predictions made by the learners. For example, in multiclass classification, we have\n",
    "$$\\tilde{y}_k =\\sum_j^L w_j d^{(j)}_k(\\boldsymbol{x}), \\text{ where }w_j\\geq 0\\text{ and }\\sum_j w_j=1,$$<p>for any class $k$, where $L$ is the number of voters. This can be simplified to the <strong>plurarity vote</strong> where each voter has the same weight:</p>\n",
    "$$\\tilde{y}_k =\\sum_j \\frac{1}{L} d^{(j)}_k(\\boldsymbol{x}).$$<p> We use the <code>VotingClassifier</code> from Scikit-learn to combine several classifiers.</p>\n",
    "\n",
    "We will use the Sklearn <code>Pipeline</code> tools which allow to combine all the steps we have seen previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LICaKWRcSNBY"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe1 = Pipeline([['sc', StandardScaler()], ['clf', LogisticRegression(**{'C': 0.1, 'fit_intercept': True, 'penalty': 'l1', 'solver': 'saga'}, random_state=random_state)]])\n",
    "pipe2 = Pipeline([['clf', DecisionTreeClassifier(**{'class_weight': None, 'criterion': 'gini', 'max_depth': 80, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'best'}, random_state=random_state)]])\n",
    "pipe3 = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsClassifier(**{'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'})]])\n",
    "pipe4 = Pipeline([['sc', StandardScaler()], ['clf', LinearDiscriminantAnalysis(**{'shrinkage': 0.7, 'solver': 'lsqr'})]])\n",
    "pipe5 = Pipeline([['sc', StandardScaler()], ['clf', GaussianNB(**{'var_smoothing': 1e-09})]])\n",
    "pipe6 = Pipeline([['sc', StandardScaler()], ['clf', SVC(**{'C': 0.5, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'probability': True}, random_state=random_state)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ARQMF5DSNBe"
   },
   "source": [
    "We can estimate the performance of individual classifiers via the 5-fold CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPDX2-OASNBj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "clf_labels = ['LR', 'DTC', 'KNN', 'LDA', 'GNB', 'SVC']\n",
    "print('[Individual]')\n",
    "for pipe, label in zip([pipe1, pipe2, pipe3, pipe4, pipe5, pipe6], clf_labels):\n",
    "    results = cross_validate(estimator=pipe, X=x_train, y=y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    scores_val = results['test_score']\n",
    "    scores_train = results['train_score']\n",
    "    print('%s: train = %.3f (+/- %.3f), validation = %.3f (+/- %.3f)' % (label, scores_train.mean(), scores_train.std(), scores_val.mean(), scores_val.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJotcstcSNBo"
   },
   "source": [
    "We combine the classifiers by <code>VotingClassifer</code> from Scikit-learn and experiment some weight combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zlu_2z9WSNBp"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "import itertools\n",
    "\n",
    "print('[Voting]')\n",
    "best_vt, best_w, best_val_score, best_train_score = None, (), -1, 1\n",
    "for a, b, c in list(itertools.permutations(range(0,3))): # try some weight combination\n",
    "    clf = VotingClassifier(estimators=[('dtc', pipe2), ('knn', pipe3), ('svc', pipe6)],\n",
    "                           voting='soft', weights=[a,b,c])\n",
    "    results = cross_validate(estimator=clf, X=x_train, y=y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    scores_val = results['test_score']\n",
    "    scores_train = results['train_score']\n",
    "    print('%s: train = %.3f (+/- %.3f), validation = %.3f (+/- %.3f)' % ((a,b,c), scores_train.mean(), scores_train.std(), scores_val.mean(), scores_val.std()))\n",
    "    if best_val_score < scores_val.mean() and best_train_score > scores_train.mean():\n",
    "        best_vt, best_w, best_val_score = clf, (a, b, c), scores_val.mean()\n",
    "\n",
    "print('\\nBest %s: %.3f' % (best_w, best_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNd7Zji6SNBu"
   },
   "source": [
    "### Exercise 5\n",
    "What is the best ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gabgxFJFSNBv"
   },
   "source": [
    "# 6) Final prediction and Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIoe_ZlYSNBw"
   },
   "source": [
    "We have now reached the final step of our ML pipeline : evaluation on the hold out test set, which has remained untouched so far. Our final model will be an ensemble which combines the <code>KNeighborsClassifier</code> and <code>SVC</code> with a z-score standardization as feature preprocessing. Let us fit the model on all the training data as follows:\n",
    "\n",
    "*Note : A decision tree does not need a z-score preprocessing because by nature the algorithm is scale invariant.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KannU_dNSNBx"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, auc, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "pipe3 = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsClassifier(**{'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'})]])\n",
    "pipe6 = Pipeline([['sc', StandardScaler()], ['clf', SVC(**{'C': 0.5000000000000001, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'probability': True})]])\n",
    "\n",
    "clf = VotingClassifier(estimators=[('knn', pipe3), ('svc', pipe6)], voting='soft', weights=[1,2])\n",
    "clf.fit(x_train, y_train)\n",
    "classes = clf.classes_ #number of classes\n",
    "\n",
    "# Here we have the probabilty associate to each classes\n",
    "proba_test = clf.predict_proba(x_test)[:,1] # [:,1] referes to the second class HGG\n",
    "y_pred = np.where(proba_test>0.5, 1, 0) # Here we have the prediction\n",
    "\n",
    "\n",
    "# 1 -- Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 2 -- ROC curve\n",
    "fp_rates, tp_rates, _ = roc_curve(y_test, proba_test, pos_label=1)\n",
    "roc_auc = auc(fp_rates, tp_rates)\n",
    "\n",
    "tn, fp, fn, tp = [i for i in cm.ravel()]\n",
    "\n",
    "# 3 -- Calculate each metrics\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
    "\n",
    "printout = (\n",
    "        f'Precision: {round(precision, 3)} | '\n",
    "        f'Recall: {round(recall, 3)} | '\n",
    "        f'F1 Score: {round(F1, 3)} | '\n",
    "        f'Accuracy Score: {round(accuracy, 3)} | '\n",
    "        f'ROC auc: {round(roc_auc, 3)} | '\n",
    "\n",
    "    )\n",
    "print(printout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUKDgBwBASrd"
   },
   "source": [
    "Now, we evaluate the classifier using different metrics.\n",
    "Let us plot the confusion matrix and ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-skpMrqAQ11"
   },
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(clf, x_test, y_test);\n",
    "RocCurveDisplay.from_estimator(clf, x_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3ExnZBKSNCC"
   },
   "source": [
    "Finally, let's look at the distribution of predicted probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BSCoEchSNCD"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'probPos': proba_test, 'target': y_test})\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(df[df.target == 0].probPos, density=True, bins=25,\n",
    "             alpha=.5, color='green', label='LGG')\n",
    "plt.hist(df[df.target == 1].probPos, density=True, bins=25,\n",
    "             alpha=.5, color='red', label='HGG')\n",
    "plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n",
    "plt.xlim([0, 1])\n",
    "plt.title('Distributions of Predictions', size=15)\n",
    "plt.xlabel('Positive Probability (predicted)', size=13)\n",
    "plt.ylabel('Samples (normalized scale)', size=13)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
